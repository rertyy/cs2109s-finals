{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# def get_augmentations():\n",
    "\n",
    "#     return transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "\n",
    "#                                transforms.RandomVerticalFlip(),\n",
    "\n",
    "#                                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "\n",
    "#                                ])\n",
    "\n",
    "def generate_synthetic(X, labels, n_neighbors=3):\n",
    "\n",
    "    X = X.copy()\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    X_where_y0 = X[labels == 0]  # majority class\n",
    "\n",
    "    X_where_y1 = X[labels == 1]\n",
    "\n",
    "    X_where_y2 = X[labels == 2]\n",
    "\n",
    "    y0_num = X_where_y0.shape[0]\n",
    "\n",
    "    y1_num = X_where_y1.shape[0]\n",
    "\n",
    "    y2_num = X_where_y2.shape[0]\n",
    "\n",
    "    X_w_y1_reshaped = X_where_y1.reshape(X_where_y1.shape[0], -1)\n",
    "\n",
    "    X_w_y2_reshaped = X_where_y2.reshape(X_where_y2.shape[0], -1)\n",
    "\n",
    "    y1_upsample = y0_num - y1_num\n",
    "\n",
    "    y2_upsample = y0_num - y2_num\n",
    "\n",
    "    X_w_y1_synthetic = smote(X_w_y1_reshaped, y1_upsample, n_neighbors)\n",
    "\n",
    "    X_w_y2_synthetic = smote(X_w_y2_reshaped, y2_upsample, n_neighbors)\n",
    "\n",
    "    X_w_y1_synthetic = X_w_y1_synthetic.reshape(-1, *X_where_y1.shape[1:])\n",
    "\n",
    "    X_w_y2_synthetic = X_w_y2_synthetic.reshape(-1, *X_where_y2.shape[1:])\n",
    "\n",
    "    X_oversampled = np.vstack([X, X_w_y1_synthetic, X_w_y2_synthetic])\n",
    "\n",
    "    y_oversampled = np.hstack([\n",
    "\n",
    "        labels,\n",
    "\n",
    "        np.ones(X_w_y1_synthetic.shape[0]),\n",
    "\n",
    "        np.full(X_w_y2_synthetic.shape[0], 2)\n",
    "\n",
    "    ])\n",
    "\n",
    "    return X_oversampled, y_oversampled\n",
    "\n",
    "def smote(X, num_oversamples, n_neighbors=5):\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    synthetic_samples = np.zeros((num_oversamples, n_features))\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree')\n",
    "\n",
    "    nn.fit(X)\n",
    "\n",
    "    indices = np.random.randint(0, n_samples, size=num_oversamples)\n",
    "\n",
    "    samples = X[indices]\n",
    "\n",
    "    nnres = nn.kneighbors(samples, return_distance=False)\n",
    "\n",
    "    nn_indices = nnres[np.arange(num_oversamples), np.random.randint(0, n_neighbors, size=num_oversamples)]\n",
    "\n",
    "    nn_samples = X[nn_indices]\n",
    "\n",
    "    diffs = nn_samples - samples\n",
    "\n",
    "    synthetic_samples = samples + diffs * np.random.random(size=(num_oversamples, 1))\n",
    "\n",
    "    return synthetic_samples.reshape(num_oversamples, *X.shape[1:])\n",
    "\n",
    "def drop_nan_y(X, y):\n",
    "\n",
    "    nan_indices = np.argwhere(np.isnan(y)).squeeze()\n",
    "\n",
    "    mask = np.ones(y.shape, bool)\n",
    "\n",
    "    mask[nan_indices] = False\n",
    "\n",
    "    X = X[mask]\n",
    "\n",
    "    y = y[mask]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def clean_x_data(X):\n",
    "\n",
    "    X[np.isnan(X)] = np.nanmedian(X)\n",
    "\n",
    "    X[X < 0] = 0\n",
    "\n",
    "    X[X > 255] = 255\n",
    "\n",
    "    # lower = np.percentile(X, 25) * 1.15\n",
    "\n",
    "    # upper = np.percentile(X, 75) * 1.5\n",
    "\n",
    "    # X[X < lower] = lower\n",
    "\n",
    "    # X[X > upper] = upper\n",
    "\n",
    "    return X\n",
    "\n",
    "# class CustomNeuralNetwork(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_size, classes=3, drop_prob=0.5):\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.network = nn.Sequential(\n",
    "\n",
    "#             nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3),\n",
    "\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "#             nn.Flatten(),\n",
    "\n",
    "#         )\n",
    "\n",
    "#\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "\n",
    "#             nn.Linear(256, 128),\n",
    "\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Linear(128, 64),  # New fully connected layer\n",
    "\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Linear(64, 32),\n",
    "\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Linear(32, classes)\n",
    "\n",
    "#         )\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = self.network(x)\n",
    "\n",
    "#         # print(x.shape)\n",
    "\n",
    "#         x = self.fc(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "class CustomNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, classes=3, drop_prob=0.3):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(drop_prob),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # nn.Dropout(drop_prob),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "\n",
    "            nn.Linear(256, 64),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, classes)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.network(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Model:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    This class represents an AI model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "\n",
    "                 batch_size=10,\n",
    "\n",
    "                 epochs=10,  # epochs seem to get worse after about 10 at num_components=256\n",
    "\n",
    "                 # learning_rate=1e-3,\n",
    "\n",
    "                 criterion=nn.CrossEntropyLoss,\n",
    "\n",
    "                 num_components=256,\n",
    "\n",
    "                 scaler=MinMaxScaler(),\n",
    "\n",
    "                 learning_rate=1e-3,\n",
    "\n",
    "                 ):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Constructor for Model class.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "        ----------\n",
    "\n",
    "        self : object\n",
    "\n",
    "            The instance of the object passed by Python.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Replace the following code with your own initialization code.\n",
    "\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.optimizer = None\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.criterion = criterion()\n",
    "\n",
    "        self.num_components = num_components\n",
    "\n",
    "        self.pca = PCA(n_components=num_components, svd_solver='full')\n",
    "\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Train the model using the input data.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "        ----------\n",
    "\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "\n",
    "            Training data.\n",
    "\n",
    "        y : ndarray of shape (n_samples,)\n",
    "\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "\n",
    "        -------\n",
    "\n",
    "        self : object\n",
    "\n",
    "            Returns an instance of the trained model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Add your training code.\n",
    "\n",
    "        self.model = CustomNeuralNetwork(input_size=self.num_components)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        print('start')\n",
    "\n",
    "        X, y = drop_nan_y(X, y)\n",
    "\n",
    "        X = clean_x_data(X)\n",
    "\n",
    "        # print(\"pre-synthetic\")\n",
    "\n",
    "        X, y = generate_synthetic(X, y, 5)\n",
    "\n",
    "        # print(y.min())\n",
    "\n",
    "        # X, X_test, y, y_test = train_test_split(X, y, test_size=100)\n",
    "\n",
    "        # print(y.min())\n",
    "\n",
    "        # Flatten and normalize the data\n",
    "\n",
    "        flattened_data = X.reshape(X.shape[0], -1)\n",
    "\n",
    "        normalized_data = self.scaler.fit_transform(flattened_data)\n",
    "\n",
    "        # print(\"pre-pca\")\n",
    "\n",
    "        # print(y.min())\n",
    "\n",
    "        pca_result = self.pca.fit_transform(normalized_data)\n",
    "\n",
    "        reconstructed = self.pca.inverse_transform(pca_result)\n",
    "\n",
    "        original_pca = reconstructed.reshape(-1, *X.shape[1:])\n",
    "\n",
    "        pca_result_tensor = torch.tensor(original_pca, dtype=torch.float32)  #.to(self.device)\n",
    "\n",
    "        labels_tensor = torch.tensor(y, dtype=torch.long)  # .to(self.device)\n",
    "\n",
    "        # print(y.min())\n",
    "\n",
    "        # dataset = CustomTensorDataset(tensors=(pca_result_tensor, labels_tensor), transform=get_augmentations())\n",
    "\n",
    "        dataset = TensorDataset(pca_result_tensor, labels_tensor)\n",
    "\n",
    "        train_loader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # print(\"pre-epoch\")\n",
    "\n",
    "        epoch_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            epoch_loss = 0\n",
    "\n",
    "            # print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "\n",
    "                # print(inputs, labels)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} loss: {epoch_losses[-1]}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Use the trained model to make predictions.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "        ----------\n",
    "\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "\n",
    "        -------\n",
    "\n",
    "        ndarray of shape (n_samples,)\n",
    "\n",
    "        Predicted target values per element in X.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Replace the following code with your own prediction code.\n",
    "\n",
    "        X = clean_x_data(X)\n",
    "\n",
    "        X = torch.from_numpy(X).float()\n",
    "\n",
    "        # X.to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        flattened_data = X.reshape(X.shape[0], -1)\n",
    "\n",
    "        normalized_data = self.scaler.transform(flattened_data)\n",
    "\n",
    "        pca_result = self.pca.transform(normalized_data)\n",
    "\n",
    "        reconstructed = self.pca.inverse_transform(pca_result)\n",
    "\n",
    "        original_pca = reconstructed.reshape(-1, *X.shape[1:])\n",
    "\n",
    "        print(\"fit shape:\", pca_result.shape)\n",
    "\n",
    "        original_pca = torch.tensor(original_pca, dtype=torch.float32)  #.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = self.model(original_pca)\n",
    "\n",
    "        return outputs.detach().numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.41708080986310603\n",
      "Epoch 2 loss: 0.16846878853494324\n",
      "Epoch 3 loss: 0.12867764607753884\n",
      "Epoch 4 loss: 0.0952498455428415\n",
      "Epoch 5 loss: 0.07896771003108236\n",
      "Epoch 6 loss: 0.07571414464089482\n",
      "Epoch 7 loss: 0.05718140528039873\n",
      "Epoch 8 loss: 0.05054400922858523\n",
      "Epoch 9 loss: 0.05077423851944079\n",
      "Epoch 10 loss: 0.04468724971675406\n",
      "fit shape: (262, 256)\n",
      "y_test values: (235,) (26,) (1,)\n",
      "predictions: (230,) (29,) (3,)\n",
      "f1: 0.7166829586184426\n",
      "Fold: 2\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.37655568522291244\n",
      "Epoch 2 loss: 0.17109328936366075\n",
      "Epoch 3 loss: 0.12864144107613204\n",
      "Epoch 4 loss: 0.10938909662068931\n",
      "Epoch 5 loss: 0.08754905897083473\n",
      "Epoch 6 loss: 0.082910882219847\n",
      "Epoch 7 loss: 0.06899491239625452\n",
      "Epoch 8 loss: 0.062298821582082595\n",
      "Epoch 9 loss: 0.05867836138243756\n",
      "Epoch 10 loss: 0.04650756284907187\n",
      "fit shape: (262, 256)\n",
      "y_test values: (246,) (16,) (0,)\n",
      "predictions: (232,) (28,) (2,)\n",
      "f1: 0.48326359832635984\n",
      "Fold: 3\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.3858720861164847\n",
      "Epoch 2 loss: 0.1801535861729572\n",
      "Epoch 3 loss: 0.13931691727778317\n",
      "Epoch 4 loss: 0.11634194795885656\n",
      "Epoch 5 loss: 0.09660463322081239\n",
      "Epoch 6 loss: 0.08376060168161713\n",
      "Epoch 7 loss: 0.06091921356500879\n",
      "Epoch 8 loss: 0.06272459589344752\n",
      "Epoch 9 loss: 0.04910659817935489\n",
      "Epoch 10 loss: 0.04519771562134805\n",
      "fit shape: (262, 256)\n",
      "y_test values: (241,) (19,) (2,)\n",
      "predictions: (235,) (26,) (1,)\n",
      "f1: 0.5605664488017429\n",
      "Fold: 4\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.3906065149221212\n",
      "Epoch 2 loss: 0.1797739231465932\n",
      "Epoch 3 loss: 0.13845581477169155\n",
      "Epoch 4 loss: 0.12418588687685223\n",
      "Epoch 5 loss: 0.10530152045757633\n",
      "Epoch 6 loss: 0.08916038991746437\n",
      "Epoch 7 loss: 0.07746287490833759\n",
      "Epoch 8 loss: 0.0660756595441138\n",
      "Epoch 9 loss: 0.05811585006593491\n",
      "Epoch 10 loss: 0.057546486354439486\n",
      "fit shape: (262, 256)\n",
      "y_test values: (239,) (19,) (4,)\n",
      "predictions: (234,) (28,) (0,)\n",
      "f1: 0.5468939768791327\n",
      "Fold: 5\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.4043628249035711\n",
      "Epoch 2 loss: 0.18963471979023544\n",
      "Epoch 3 loss: 0.1519892946012134\n",
      "Epoch 4 loss: 0.11802197359128637\n",
      "Epoch 5 loss: 0.10952572011817131\n",
      "Epoch 6 loss: 0.0886951913260553\n",
      "Epoch 7 loss: 0.08037213174560029\n",
      "Epoch 8 loss: 0.07026443102022538\n",
      "Epoch 9 loss: 0.06936649063914327\n",
      "Epoch 10 loss: 0.05628648493782546\n",
      "fit shape: (262, 256)\n",
      "y_test values: (233,) (25,) (4,)\n",
      "predictions: (232,) (28,) (2,)\n",
      "f1: 0.7417596537499155\n",
      "Fold: 6\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.4056770771348067\n",
      "Epoch 2 loss: 0.1815958862397015\n",
      "Epoch 3 loss: 0.13876644169148575\n",
      "Epoch 4 loss: 0.1165084756569819\n",
      "Epoch 5 loss: 0.09810776101994072\n",
      "Epoch 6 loss: 0.08865318952954157\n",
      "Epoch 7 loss: 0.07340316867276546\n",
      "Epoch 8 loss: 0.06175360567455901\n",
      "Epoch 9 loss: 0.05835926241150693\n",
      "Epoch 10 loss: 0.046204452459435594\n",
      "fit shape: (262, 256)\n",
      "y_test values: (234,) (24,) (4,)\n",
      "predictions: (233,) (25,) (4,)\n",
      "f1: 0.7573926786988885\n",
      "Fold: 7\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.39046682912274805\n",
      "Epoch 2 loss: 0.17166054278498422\n",
      "Epoch 3 loss: 0.13429355265821333\n",
      "Epoch 4 loss: 0.11015689979077761\n",
      "Epoch 5 loss: 0.09181285051995242\n",
      "Epoch 6 loss: 0.0771299760650672\n",
      "Epoch 7 loss: 0.06961672022016234\n",
      "Epoch 8 loss: 0.05755291305758642\n",
      "Epoch 9 loss: 0.05196164743770583\n",
      "Epoch 10 loss: 0.043645317274185545\n",
      "fit shape: (262, 256)\n",
      "y_test values: (238,) (21,) (3,)\n",
      "predictions: (240,) (20,) (2,)\n",
      "f1: 0.782569649964282\n",
      "Fold: 8\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.38433929350250184\n",
      "Epoch 2 loss: 0.18263327859734235\n",
      "Epoch 3 loss: 0.14790507503705902\n",
      "Epoch 4 loss: 0.12260807122337421\n",
      "Epoch 5 loss: 0.11131197283254943\n",
      "Epoch 6 loss: 0.09070152153313987\n",
      "Epoch 7 loss: 0.0761299543091274\n",
      "Epoch 8 loss: 0.067814007497014\n",
      "Epoch 9 loss: 0.05446859878625655\n",
      "Epoch 10 loss: 0.05388011147643223\n",
      "fit shape: (262, 256)\n",
      "y_test values: (241,) (19,) (2,)\n",
      "predictions: (225,) (33,) (4,)\n",
      "f1: 0.7463776090385532\n",
      "Fold: 9\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.4069620802372833\n",
      "Epoch 2 loss: 0.18184713897560353\n",
      "Epoch 3 loss: 0.1473363023986359\n",
      "Epoch 4 loss: 0.1148977182196485\n",
      "Epoch 5 loss: 0.10172852042256413\n",
      "Epoch 6 loss: 0.08706540630708581\n",
      "Epoch 7 loss: 0.07411452646253733\n",
      "Epoch 8 loss: 0.06686047868036667\n",
      "Epoch 9 loss: 0.06206031295747538\n",
      "Epoch 10 loss: 0.05023049482248214\n",
      "fit shape: (262, 256)\n",
      "y_test values: (249,) (10,) (3,)\n",
      "predictions: (240,) (19,) (3,)\n",
      "f1: 0.7930094257574689\n",
      "Fold: 10\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.3883751932088245\n",
      "Epoch 2 loss: 0.1756819037244104\n",
      "Epoch 3 loss: 0.13637260515696242\n",
      "Epoch 4 loss: 0.11449185715375794\n",
      "Epoch 5 loss: 0.09451070991816168\n",
      "Epoch 6 loss: 0.07267573660275924\n",
      "Epoch 7 loss: 0.06933518956718358\n",
      "Epoch 8 loss: 0.057517839928606755\n",
      "Epoch 9 loss: 0.04900322340432844\n",
      "Epoch 10 loss: 0.050523528969248215\n",
      "fit shape: (262, 256)\n",
      "y_test values: (236,) (24,) (2,)\n",
      "predictions: (234,) (27,) (1,)\n",
      "f1: 0.7215408149075233\n",
      "F1: [0.7166829586184426, 0.48326359832635984, 0.5605664488017429, 0.5468939768791327, 0.7417596537499155, 0.7573926786988885, 0.782569649964282, 0.7463776090385532, 0.7930094257574689, 0.7215408149075233]\n",
      "Mean: 0.6850056814742309\n",
      "Std: 0.10536082787513698\n",
      "Max: 0.7930094257574689\n",
      "Min: 0.48326359832635984\n",
      "peak memory: 597.02 MiB, increment: 319.86 MiB\n",
      "CPU times: total: 31min 9s\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# N fold cross validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']\n",
    "\n",
    "\n",
    "nan_indices = np.argwhere(np.isnan(y)).squeeze()\n",
    "mask = np.ones(y.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "model = Model()\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=2109)\n",
    "\n",
    "f1_scores = []\n",
    "i = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    i += 1\n",
    "    print(\"Fold:\", i)\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"y_test values:\", y_test[y_test==0].shape, y_test[y_test==1].shape, y_test[y_test==2].shape)\n",
    "    print(\"predictions:\", predictions[predictions==0].shape, predictions[predictions==1].shape, predictions[predictions==2].shape)\n",
    "\n",
    "    score = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "    f1_scores.append(score)\n",
    "    print(\"f1:\", score)\n",
    "\n",
    "print(\"F1:\", f1_scores)\n",
    "print(\"Mean:\", np.mean(f1_scores))\n",
    "print(\"Std:\", np.std(f1_scores))\n",
    "print(\"Max:\", np.max(f1_scores))\n",
    "print(\"Min:\", np.min(f1_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs2109s-2310-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
