{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# def get_augmentations():\n",
    "#     return transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "#                                transforms.RandomVerticalFlip(),\n",
    "#                                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "#                                ])\n",
    "\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    Copied directly from https://stackoverflow.com/questions/55588201/pytorch-transforms-on-tensordataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "\n",
    "def generate_synthetic(X, labels, n_neighbors=3):\n",
    "    X = X.copy()\n",
    "    print(X.shape)\n",
    "    X_where_y0 = X[labels == 0]  # majority class\n",
    "    X_where_y1 = X[labels == 1]\n",
    "    X_where_y2 = X[labels == 2]\n",
    "    y0_num = X_where_y0.shape[0]\n",
    "    y1_num = X_where_y1.shape[0]\n",
    "    y2_num = X_where_y2.shape[0]\n",
    "\n",
    "    X_w_y1_reshaped = X_where_y1.reshape(X_where_y1.shape[0], -1)\n",
    "    X_w_y2_reshaped = X_where_y2.reshape(X_where_y2.shape[0], -1)\n",
    "\n",
    "    y1_upsample = y0_num - y1_num\n",
    "    y2_upsample = y0_num - y2_num\n",
    "\n",
    "    X_w_y1_synthetic = smote(X_w_y1_reshaped, y1_upsample, n_neighbors)\n",
    "    X_w_y2_synthetic = smote(X_w_y2_reshaped, y2_upsample, n_neighbors)\n",
    "\n",
    "    X_w_y1_synthetic = X_w_y1_synthetic.reshape(-1, *X_where_y1.shape[1:])\n",
    "    X_w_y2_synthetic = X_w_y2_synthetic.reshape(-1, *X_where_y2.shape[1:])\n",
    "\n",
    "    X_oversampled = np.vstack([X, X_w_y1_synthetic, X_w_y2_synthetic])\n",
    "    y_oversampled = np.hstack([\n",
    "        labels,\n",
    "        np.ones(X_w_y1_synthetic.shape[0]),\n",
    "        np.full(X_w_y2_synthetic.shape[0], 2)\n",
    "    ])\n",
    "\n",
    "    return X_oversampled, y_oversampled\n",
    "\n",
    "\n",
    "def smote(X, num_oversamples, n_neighbors=5, seed=2109):\n",
    "    np.random.seed(seed)\n",
    "    n_samples, n_features = X.shape\n",
    "    synthetic_samples = np.zeros((num_oversamples, n_features))\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree')\n",
    "    nn.fit(X)\n",
    "\n",
    "    indices = np.random.randint(0, n_samples, size=num_oversamples)\n",
    "    samples = X[indices]\n",
    "\n",
    "    nnres = nn.kneighbors(samples, return_distance=False)\n",
    "\n",
    "    nn_indices = nnres[np.arange(num_oversamples), np.random.randint(0, n_neighbors, size=num_oversamples)]\n",
    "    nn_samples = X[nn_indices]\n",
    "\n",
    "    diffs = nn_samples - samples\n",
    "    synthetic_samples = samples + diffs * np.random.random(size=(num_oversamples, 1))\n",
    "\n",
    "    return synthetic_samples.reshape(num_oversamples, *X.shape[1:])\n",
    "\n",
    "\n",
    "def drop_nan_y(X, y):\n",
    "    nan_indices = np.argwhere(np.isnan(y)).squeeze()\n",
    "    mask = np.ones(y.shape, bool)\n",
    "    mask[nan_indices] = False\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def clean_x_data(X):\n",
    "    X[np.isnan(X)] = np.nanmedian(X)\n",
    "    X[X < 0] = 0\n",
    "    X[X > 255] = 255\n",
    "    # lower = np.percentile(X, 25) * 1.15\n",
    "    # upper = np.percentile(X, 75) * 1.5\n",
    "    # X[X < lower] = lower\n",
    "    # X[X > upper] = upper\n",
    "    return X\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "\n",
    "# class CustomNeuralNetwork(nn.Module):\n",
    "#     def __init__(self, input_size, classes=3, drop_prob=0.5):\n",
    "#         super().__init__()\n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Flatten(),\n",
    "#         )\n",
    "#\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),  # New fully connected layer\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, classes)\n",
    "#         )\n",
    "#\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.network(x)\n",
    "#         # print(x.shape)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "class CustomNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, classes=3, drop_prob=0.3, seed=2109):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(drop_prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size=20,\n",
    "                 epochs=10,  # epochs seem to get worse after about 10 at num_components=256\n",
    "                 # learning_rate=1e-3,\n",
    "                 criterion=nn.CrossEntropyLoss,\n",
    "                 num_components=256,\n",
    "                 scaler=MinMaxScaler(),\n",
    "                 learning_rate=0.0003826645125269827,\n",
    "                 dropout=0.23535222860200122,\n",
    "                 seed = 2109\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following code with your own initialization code.\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        self.optimizer = None\n",
    "        self.model = None\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.criterion = criterion()\n",
    "        self.num_components = num_components\n",
    "        self.pca = PCA(n_components=num_components, svd_solver='full')\n",
    "        self.scaler = scaler\n",
    "        self.dropout = dropout\n",
    "        self.seed = seed\n",
    "\n",
    "        self.g = torch.Generator()\n",
    "        self.g.manual_seed(self.seed)\n",
    "\n",
    "\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "        # TODO: Add your training code.\n",
    "\n",
    "        self.model = CustomNeuralNetwork(input_size=self.num_components, drop_prob=self.dropout)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        print('start')\n",
    "\n",
    "        X, y = drop_nan_y(X, y)\n",
    "\n",
    "        X = clean_x_data(X)\n",
    "\n",
    "        # print(\"pre-synthetic\")\n",
    "        X, y = generate_synthetic(X, y, 5)\n",
    "        # print(y.min())\n",
    "\n",
    "        # X, X_test, y, y_test = train_test_split(X, y, test_size=100)\n",
    "        # print(y.min())\n",
    "\n",
    "        # Flatten and normalize the data\n",
    "        flattened_data = X.reshape(X.shape[0], -1)\n",
    "\n",
    "        normalized_data = self.scaler.fit_transform(flattened_data)\n",
    "        # print(\"pre-pca\")\n",
    "        # print(y.min())\n",
    "        pca_result = self.pca.fit_transform(normalized_data)\n",
    "        reconstructed = self.pca.inverse_transform(pca_result)\n",
    "        original_pca = reconstructed.reshape(-1, *X.shape[1:])\n",
    "\n",
    "        pca_result_tensor = torch.tensor(original_pca, dtype=torch.float32)  #.to(self.device)\n",
    "        labels_tensor = torch.tensor(y, dtype=torch.long)  # .to(self.device)\n",
    "\n",
    "        # print(y.min())\n",
    "        # dataset = CustomTensorDataset(tensors=(pca_result_tensor, labels_tensor), transform=get_augmentations())\n",
    "        dataset = TensorDataset(pca_result_tensor, labels_tensor)\n",
    "        train_loader = DataLoader(dataset=dataset, \n",
    "                                  batch_size=self.batch_size, \n",
    "                                  shuffle=True,\n",
    "                                  num_workers=0,\n",
    "                                    worker_init_fn=seed_worker,\n",
    "                                    generator=self.g\n",
    "                                  )\n",
    "        # print(\"pre-epoch\")\n",
    "\n",
    "        epoch_losses = []\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "            # print(f\"Epoch {epoch+1}\")\n",
    "            for inputs, labels in train_loader:\n",
    "                # print(inputs, labels)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_losses.append(epoch_loss / len(train_loader))\n",
    "            print(f\"Epoch {epoch + 1} loss: {epoch_losses[-1]}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following code with your own prediction code.\n",
    "        X = clean_x_data(X)\n",
    "\n",
    "        X = torch.from_numpy(X).float()\n",
    "        # X.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        flattened_data = X.reshape(X.shape[0], -1)\n",
    "        normalized_data = self.scaler.transform(flattened_data)\n",
    "        pca_result = self.pca.transform(normalized_data)\n",
    "        reconstructed = self.pca.inverse_transform(pca_result)\n",
    "        original_pca = reconstructed.reshape(-1, *X.shape[1:])\n",
    "\n",
    "        print(\"fit shape:\", pca_result.shape)\n",
    "\n",
    "        original_pca = torch.tensor(original_pca, dtype=torch.float32)  #.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(original_pca)\n",
    "        return outputs.detach().numpy().argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "(2359, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6286987872454035\n",
      "Epoch 2 loss: 0.2604705119049955\n",
      "Epoch 3 loss: 0.21115822121846972\n",
      "Epoch 4 loss: 0.1739596270151644\n",
      "Epoch 5 loss: 0.15880519668384233\n",
      "Epoch 6 loss: 0.13987965994275206\n",
      "Epoch 7 loss: 0.12404102294910527\n",
      "Epoch 8 loss: 0.11697164301013707\n",
      "Epoch 9 loss: 0.10332488018338894\n",
      "Epoch 10 loss: 0.09270679534502305\n",
      "fit shape: (261, 256)\n",
      "F1 Score (macro): 0.81\n",
      "peak memory: 582.51 MiB, increment: 304.70 MiB\n",
      "CPU times: total: 1min 19s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']\n",
    "\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Filter test data that contains no labels\n",
    "# In Coursemology, the test data is guaranteed to have labels\n",
    "nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "mask = np.ones(y_test.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Train and predict\n",
    "model = Model()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"F1 Score (macro): {0:.2f}\".format(f1_score(y_test, y_pred, average='macro'))) # You may encounter errors, you are expected to figure out what's the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6426291365038466\n",
      "Epoch 2 loss: 0.2630904563415198\n",
      "Epoch 3 loss: 0.20912013877252186\n",
      "Epoch 4 loss: 0.1727606677967641\n",
      "Epoch 5 loss: 0.15446377038541767\n",
      "Epoch 6 loss: 0.13701527513402664\n",
      "Epoch 7 loss: 0.12748129640125244\n",
      "Epoch 8 loss: 0.1111683818442678\n",
      "Epoch 9 loss: 0.10759369346577628\n",
      "Epoch 10 loss: 0.09330346542956698\n",
      "fit shape: (262, 256)\n",
      "f1: 0.5894590262050629\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6299949592176062\n",
      "Epoch 2 loss: 0.2629056622083328\n",
      "Epoch 3 loss: 0.21402566529440215\n",
      "Epoch 4 loss: 0.18143489352363792\n",
      "Epoch 5 loss: 0.1587032848847625\n",
      "Epoch 6 loss: 0.14528984196414674\n",
      "Epoch 7 loss: 0.12717498977442984\n",
      "Epoch 8 loss: 0.11744619107672147\n",
      "Epoch 9 loss: 0.10922619391822759\n",
      "Epoch 10 loss: 0.0946389948573218\n",
      "fit shape: (262, 256)\n",
      "f1: 0.7086496156263598\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6128509369658612\n",
      "Epoch 2 loss: 0.24906947081995084\n",
      "Epoch 3 loss: 0.20886106601117566\n",
      "Epoch 4 loss: 0.18678628938163028\n",
      "Epoch 5 loss: 0.16398788086101182\n",
      "Epoch 6 loss: 0.1455663768430428\n",
      "Epoch 7 loss: 0.13589506414964012\n",
      "Epoch 8 loss: 0.12452237265351757\n",
      "Epoch 9 loss: 0.11265586530818567\n",
      "Epoch 10 loss: 0.10096903428530477\n",
      "fit shape: (262, 256)\n",
      "f1: 0.649624060150376\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6277149423910737\n",
      "Epoch 2 loss: 0.255270524544428\n",
      "Epoch 3 loss: 0.21222325003829903\n",
      "Epoch 4 loss: 0.18368053560263417\n",
      "Epoch 5 loss: 0.16443042737942176\n",
      "Epoch 6 loss: 0.1448029286225237\n",
      "Epoch 7 loss: 0.13302284862915337\n",
      "Epoch 8 loss: 0.11704413410875912\n",
      "Epoch 9 loss: 0.10532429795885234\n",
      "Epoch 10 loss: 0.09693551756839495\n",
      "fit shape: (262, 256)\n",
      "f1: 0.7024726537365567\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.5963955745905821\n",
      "Epoch 2 loss: 0.23792478616958784\n",
      "Epoch 3 loss: 0.19366143108633013\n",
      "Epoch 4 loss: 0.17161266827082008\n",
      "Epoch 5 loss: 0.14967672723448938\n",
      "Epoch 6 loss: 0.13431425767456306\n",
      "Epoch 7 loss: 0.12141046745351941\n",
      "Epoch 8 loss: 0.10565261636073069\n",
      "Epoch 9 loss: 0.10245719236425227\n",
      "Epoch 10 loss: 0.09223648602499362\n",
      "fit shape: (262, 256)\n",
      "f1: 0.7540159619629817\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6215509608112\n",
      "Epoch 2 loss: 0.2647363426692692\n",
      "Epoch 3 loss: 0.21707433982993718\n",
      "Epoch 4 loss: 0.18364488604237084\n",
      "Epoch 5 loss: 0.16459374620728653\n",
      "Epoch 6 loss: 0.14629055500708887\n",
      "Epoch 7 loss: 0.1367036527695341\n",
      "Epoch 8 loss: 0.12001585625801925\n",
      "Epoch 9 loss: 0.10953934699653384\n",
      "Epoch 10 loss: 0.10117575339685528\n",
      "fit shape: (262, 256)\n",
      "f1: 0.6938233383021825\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6572404890608641\n",
      "Epoch 2 loss: 0.25673346579028866\n",
      "Epoch 3 loss: 0.20904323518747625\n",
      "Epoch 4 loss: 0.17533435231580594\n",
      "Epoch 5 loss: 0.15722293883884025\n",
      "Epoch 6 loss: 0.14205772672883338\n",
      "Epoch 7 loss: 0.13026253377765784\n",
      "Epoch 8 loss: 0.11130243515952226\n",
      "Epoch 9 loss: 0.09728424067545774\n",
      "Epoch 10 loss: 0.08663139718636456\n",
      "fit shape: (262, 256)\n",
      "f1: 0.78379113018598\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6506468218408132\n",
      "Epoch 2 loss: 0.2571157957239786\n",
      "Epoch 3 loss: 0.20015863495829858\n",
      "Epoch 4 loss: 0.1685737836846488\n",
      "Epoch 5 loss: 0.15310331602399194\n",
      "Epoch 6 loss: 0.13934492234475104\n",
      "Epoch 7 loss: 0.1270386480089081\n",
      "Epoch 8 loss: 0.11356560423416619\n",
      "Epoch 9 loss: 0.09973245459225248\n",
      "Epoch 10 loss: 0.0950895100918486\n",
      "fit shape: (262, 256)\n",
      "f1: 0.7620835033319734\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6259161905148385\n",
      "Epoch 2 loss: 0.26143945991900397\n",
      "Epoch 3 loss: 0.2061683387605485\n",
      "Epoch 4 loss: 0.17436330491295143\n",
      "Epoch 5 loss: 0.15496618008264196\n",
      "Epoch 6 loss: 0.14372441141652118\n",
      "Epoch 7 loss: 0.12985110515439363\n",
      "Epoch 8 loss: 0.1148863748679088\n",
      "Epoch 9 loss: 0.10301700871877419\n",
      "Epoch 10 loss: 0.09815478514191574\n",
      "fit shape: (262, 256)\n",
      "f1: 0.6993071721538632\n",
      "start\n",
      "(2358, 3, 16, 16)\n",
      "Epoch 1 loss: 0.6299963245559254\n",
      "Epoch 2 loss: 0.25883733202921755\n",
      "Epoch 3 loss: 0.21369663507528144\n",
      "Epoch 4 loss: 0.17821152956673392\n",
      "Epoch 5 loss: 0.16658284017287286\n",
      "Epoch 6 loss: 0.14402872552221388\n",
      "Epoch 7 loss: 0.12824105794173607\n",
      "Epoch 8 loss: 0.1169830876270332\n",
      "Epoch 9 loss: 0.10460792646605384\n",
      "Epoch 10 loss: 0.09433336285328102\n",
      "fit shape: (262, 256)\n",
      "f1: 0.7543382701021167\n",
      "F1: [0.5894590262050629, 0.7086496156263598, 0.649624060150376, 0.7024726537365567, 0.7540159619629817, 0.6938233383021825, 0.78379113018598, 0.7620835033319734, 0.6993071721538632, 0.7543382701021167]\n",
      "Mean: 0.7097564731757452\n",
      "Std: 0.055380865080283614\n",
      "Max: 0.78379113018598\n",
      "Min: 0.5894590262050629\n",
      "peak memory: 644.57 MiB, increment: 241.04 MiB\n",
      "CPU times: total: 13min 47s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# N fold cross validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']\n",
    "\n",
    "\n",
    "nan_indices = np.argwhere(np.isnan(y)).squeeze()\n",
    "mask = np.ones(y.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "model = Model()\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=2109)\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    model.fit(X=X[train_index], y=y[train_index])\n",
    "\n",
    "    predictions = model.predict(X[test_index])\n",
    "\n",
    "    score = f1_score(y[test_index], predictions, average='macro')\n",
    "\n",
    "    f1_scores.append(score)\n",
    "    print(\"f1:\", score)\n",
    "\n",
    "print(\"F1:\", f1_scores)\n",
    "print(\"Mean:\", np.mean(f1_scores))\n",
    "print(\"Std:\", np.std(f1_scores))\n",
    "print(\"Max:\", np.max(f1_scores))\n",
    "print(\"Min:\", np.min(f1_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "913a6ed329cd7b2c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:34:55,205\tINFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:35:02,219\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-11-26 13:35:05,961\tINFO tune.py:1047 -- Total run time: 10.76 seconds (6.97 seconds for the tuning loop).\n",
      "2023-11-26 13:35:05,961\tWARNING tune.py:1062 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2023-11-26 13:35:05,979\tWARNING experiment_analysis.py:185 -- Failed to fetch metrics for 10 trial(s):\n",
      "- train_test_model_87a01_00000: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00000: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00000_0_batch_size=50,drop_prob=0.1490,epochs=10,lr=0.0029,scaler=ref_ph_908d9f2f_2023-11-26_13-34-55')\n",
      "- train_test_model_87a01_00001: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00001: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00001_1_batch_size=30,drop_prob=0.4055,epochs=20,lr=0.0523,scaler=ref_ph_b8999244_2023-11-26_13-34-55')\n",
      "- train_test_model_87a01_00002: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00002: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00002_2_batch_size=10,drop_prob=0.4143,epochs=30,lr=0.0001,scaler=ref_ph_908d9f2f_2023-11-26_13-34-55')\n",
      "- train_test_model_87a01_00003: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00003: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00003_3_batch_size=50,drop_prob=0.3555,epochs=30,lr=0.0218,scaler=ref_ph_908d9f2f_2023-11-26_13-34-55')\n",
      "- train_test_model_87a01_00004: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00004: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00004_4_batch_size=20,drop_prob=0.2548,epochs=10,lr=0.0003,scaler=ref_ph_908d9f2f_2023-11-26_13-34-56')\n",
      "- train_test_model_87a01_00005: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00005: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00005_5_batch_size=40,drop_prob=0.4917,epochs=20,lr=0.0015,scaler=ref_ph_908d9f2f_2023-11-26_13-34-56')\n",
      "- train_test_model_87a01_00006: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00006: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00006_6_batch_size=50,drop_prob=0.1514,epochs=30,lr=0.0004,scaler=ref_ph_b8999244_2023-11-26_13-34-56')\n",
      "- train_test_model_87a01_00007: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00007: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00007_7_batch_size=40,drop_prob=0.4885,epochs=10,lr=0.0014,scaler=ref_ph_908d9f2f_2023-11-26_13-34-56')\n",
      "- train_test_model_87a01_00008: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00008: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00008_8_batch_size=20,drop_prob=0.2170,epochs=30,lr=0.0153,scaler=ref_ph_b8999244_2023-11-26_13-34-56')\n",
      "- train_test_model_87a01_00009: FileNotFoundError('Could not fetch metrics for train_test_model_87a01_00009: both result.json and progress.csv were not found at C:/Users/Ian/ray_results/train_test_model_2023-11-26_13-34-55/train_test_model_87a01_00009_9_batch_size=30,drop_prob=0.2162,epochs=10,lr=0.0470,scaler=ref_ph_b8999244_2023-11-26_13-34-56')\n",
      "2023-11-26 13:35:05,979\tWARNING experiment_analysis.py:575 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ian\\OneDrive - National University of Singapore\\Y2S1\\CS2109S\\Finals\\final\\tuning.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ian/OneDrive%20-%20National%20University%20of%20Singapore/Y2S1/CS2109S/Finals/final/tuning.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m analysis \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mrun(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ian/OneDrive%20-%20National%20University%20of%20Singapore/Y2S1/CS2109S/Finals/final/tuning.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     train_test_model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ian/OneDrive%20-%20National%20University%20of%20Singapore/Y2S1/CS2109S/Finals/final/tuning.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ian/OneDrive%20-%20National%20University%20of%20Singapore/Y2S1/CS2109S/Finals/final/tuning.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ian/OneDrive%20-%20National%20University%20of%20Singapore/Y2S1/CS2109S/Finals/final/tuning.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ian/OneDrive%20-%20National%20University%20of%20Singapore/Y2S1/CS2109S/Finals/final/tuning.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m best_trial \u001b[39m=\u001b[39m analysis\u001b[39m.\u001b[39mget_best_trial(\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ian/OneDrive%20-%20National%20University%20of%20Singapore/Y2S1/CS2109S/Finals/final/tuning.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mprint\u001b[39m(best_trial\u001b[39m.\u001b[39;49mconfig)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from ray import train\n",
    "from ray.air import session\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def drop_nan_y(X, y):\n",
    "    nan_indices = np.argwhere(np.isnan(y)).squeeze()\n",
    "    mask = np.ones(y.shape, bool)\n",
    "    mask[nan_indices] = False\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    return X, y\n",
    "\n",
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']\n",
    "\n",
    "\n",
    "X, y = drop_nan_y(X, y)\n",
    "\n",
    "def train_test_model(config):\n",
    "    model = Model(batch_size=config[\"batch_size\"],\n",
    "                    epochs=config[\"epochs\"], \n",
    "                    # criterion=config[\"criterion\"],\n",
    "                    scaler=config[\"scaler\"],\n",
    "                    learning_rate=config[\"lr\"])\n",
    "    kf = KFold(n_splits=3)\n",
    "    f1_scores = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        f1 = f1_score(y_test, predictions, average='macro')\n",
    "        f1_scores.append(f1)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    train.report({\"score\": avg_f1})\n",
    "    \n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"drop_prob\": tune.uniform(0.1, 0.5),\n",
    "    \"batch_size\": tune.choice([10, 20, 30, 40, 50]),\n",
    "    \"epochs\": tune.choice([10, 20, 30]),\n",
    "    # \"criterion\": tune.choice([nn.CrossEntropyLoss, nn.MSELoss]),\n",
    "    \"scaler\": tune.choice([MinMaxScaler(), StandardScaler()])\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_test_model,\n",
    "    config=config,\n",
    "    num_samples=10,  # Number of times to sample from the hyperparameter space\n",
    "    resources_per_trial={\"cpu\": 16, \"gpu\": 1}  # Adjust based on your system's resources\n",
    "\n",
    ")\n",
    "best_trial = analysis.get_best_trial(\"score\",\"max\",\"last\")\n",
    "print(best_trial.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-11-26 13:26:34,995\tINFO tune.py:1047 -- Total run time: 583.14 seconds (583.05 seconds for the tuning loop).\n",
    "# {'lr': 0.0003826645125269827, 'drop_prob': 0.23535222860200122, 'batch_size': 20, 'epochs': 10, 'scaler': StandardScaler()}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
